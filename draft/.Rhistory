mean = mean(Time),
sd = sd(Time))
prob1_sum
ggplot(prob1, aes(x = Time)) +
geom_density(fill = "lightpink", color = "black") +
geom_hline(yintercept = 0) +
xlab("Finishing Time") +
ggtitle("2010 Boston Marathon",
subtitle = "Women aged 18-34") +
theme_minimal()
B = 50000
x = prob1 %>% pull(Time)
set.seed(20211115)
sample_means = tibble(
xbar = map_dbl(1:B, ~{
return( mean(sample(x, replace = TRUE)) )
}))
sim1_sum = sample_means %>%
summarize(n = n(),
mean = mean(xbar),
sd = sd(xbar))
sim1_sum
ggplot(sample_means, aes(x = xbar)) +
geom_density(fill = "lightpink", color = "black") +
geom_norm_density(sim1_sum$mean, sim1_sum$sd) +
geom_hline(yintercept = 0) +
xlab("Finishing Time Sample Means") +
ggtitle("2010 Boston Marathon, Women aged 18-34",
subtitle = "Sampling Distribution of xbar, n = 3557") +
theme_minimal()
B = 50000
n = 50
x = prob1 %>% pull(Time)
set.seed(20211115)
sample_means = tibble(
xbar = map_dbl(1:B, ~{
return( mean(sample(x, size = n, replace = TRUE)) )
}))
sim2_sum = sample_means %>%
summarize(n = n(),
mean = mean(xbar),
sd = sd(xbar))
sim2_sum
ggplot(sample_means, aes(x = xbar)) +
geom_density(fill = "lightpink", color = "black") +
geom_norm_density(sim2_sum$mean, sim2_sum$sd) +
geom_hline(yintercept = 0) +
xlab("Finishing Time Sample Means") +
ggtitle("2010 Boston Marathon, Women aged 18-34",
subtitle = "Sampling Distribution of xbar, n = 50") +
theme_minimal()
z = qnorm(0.975)
ci = prob1_sum$mean + c(-1,1)*z*sim1_sum$sd
round(ci,3)
## Simulation SE
sim1_sum$sd
## Formula
prob1_sum$sd / sqrt(prob1_sum$n)
```{r}
t.test(x)
x
capture_sim = capture_sim %>%
mutate(capture = low < mu & mu < high) %>%
mutate(index = row_number()) %>%
relocate(index)
B = 50000
n = 50
t_mult = qt(0.975, n-1)
x = prob1$Time
mu = mean(x)
sigma = sd(x) * sqrt(3556/3557)
mu
sigma
capture_base = vector("list", B)
capture_sim = map_dfr(capture_base, ~{
samp = sample(x, size = n, replace = TRUE)
out = tibble(xbar = mean(samp),
sd = sd(samp),
se = sd/sqrt(n),
low = xbar - t_mult*se,
high = xbar + t_mult*se)
return ( out )
})
capture_sim = capture_sim %>%
mutate(capture = low < mu & mu < high) %>%
mutate(index = row_number()) %>%
relocate(index)
## new sample
n = 50
new_x = sample(x, size = n)
new_x_mean = mean(new_x)
new_x_sd = sd(new_x)
new_x_se = new_x_sd / sqrt(n)
t_mult = qt(0.975, n-1)
new_x_mean
new_x_sd
new_x_se
t_mult
new_ci = new_x_mean + c(-1,1) * t_mult * new_x_se
new_ci
## compare to t.test
t.test(new_x)
B = 50000
n = 50
t_mult = qt(0.975, n-1)
x = prob1$Time
mu = mean(x)
sigma = sd(x) * sqrt(3556/3557)
mu
sigma
capture_base = vector("list", B)
capture_sim = map_dfr(capture_base, ~{
samp = sample(x, size = n, replace = TRUE)
out = tibble(xbar = mean(samp),
sd = sd(samp),
se = sd/sqrt(n),
low = xbar - t_mult*se,
high = xbar + t_mult*se)
return ( out )
})
?qbinom
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE,
cache=TRUE, autodep=TRUE, cache.comments=FALSE)
library(tidyverse)
library(httr)
library(rvest)
request = GET(url = path, query = parameters)
setwd("~/Projects/STAT_240/project/draft")
library(tidyverse)
library(httr)
library(rvest)
races_min = 2500
races_max = 3000
texts_min = 1000
racers = 100
path = "http://typeracerdata.com/leaders";
# Some parameters like the min/maxes can be filtered in R, but filtering
# helps reduce the initial data set received from the GET request
parameters = list(
min_races = races_min,
min_texts = texts_min,
rank_start = 0,
# Arbitrary large number to pull all data. If rank_end is not specified,
# the amount returned defaults to 50. There are only ~16k users logged
# in typeracerdata, so this number guarantees full retrieval
rank_end = 20000,
sort = "races"
)
request = GET(url = path, query = parameters)
request = GET(url = path, query = parameters)
request$status_code
request$status_code
table_html = read_html(request)
# Parsing rest of the table
table = table_html %>% html_table(fill = TRUE)
# Parsing rest of the table
table = table_html %>% html_table(fill = TRUE)
View(table)
table = table[[1]]
table = table %>%
select(Racer, Races, Texts) %>%
rename(
racer = Racer,
races = Races,
texts = Texts
) %>%
mutate(
racer = names,
races = parse_number(races),
texts = parse_number(texts)
)
request = GET(url = path, query = parameters)
request$status_code
table_html = read_html(request)
# Getting names from href tags
names = table_html %>%
html_nodes(xpath = "//td/a") %>%
html_attr("href") %>%
str_extract("\\=.*$") %>%
str_sub(2)
head(names)
# Parsing rest of the table
table = table_html %>% html_table(fill = TRUE)
table = table[[1]]
table = table %>%
select(Racer, Races, Texts) %>%
rename(
racer = Racer,
races = Races,
texts = Texts
) %>%
mutate(
racer = names,
races = parse_number(races),
texts = parse_number(texts)
)
head(table)
table = table %>%
# Some of these are redundant
filter(races < races_max & races > races_min & texts > texts_min) %>%
select(racer, races) #%>%
table
races_min = 2500
races_max = 3000
texts_min = 500
racers = 100
path = "http://typeracerdata.com/leaders";
# Some parameters like the min/maxes can be filtered in R, but filtering
# helps reduce the initial data set received from the GET request
parameters = list(
min_races = races_min,
min_texts = texts_min,
rank_start = 0,
# Arbitrary large number to pull all data. If rank_end is not specified,
# the amount returned defaults to 50. There are only ~16k users logged
# in typeracerdata, so this number guarantees full retrieval
rank_end = 20000,
sort = "races"
)
request = GET(url = path, query = parameters)
request$status_code
table_html = read_html(request)
# Getting names from href tags
names = table_html %>%
html_nodes(xpath = "//td/a") %>%
html_attr("href") %>%
str_extract("\\=.*$") %>%
str_sub(2)
head(names)
# Parsing rest of the table
table = table_html %>% html_table(fill = TRUE)
table = table[[1]]
table = table %>%
select(Racer, Races, Texts) %>%
rename(
racer = Racer,
races = Races,
texts = Texts
) %>%
mutate(
racer = names,
races = parse_number(races),
texts = parse_number(texts)
)
head(table)
table = table %>%
# Some of these are redundant
filter(races < races_max & races > races_min & texts > texts_min) %>%
select(racer, races) #%>%
table
str_c("a", "b")
setwd("~/Projects/STAT_240/project/draft")
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE,
cache=TRUE, autodep=TRUE, cache.comments=FALSE)
library(tidyverse)
library(httr)
library(rvest)
library(jsonlite)
id = "cheffray"
races = 2500
id = "cheffray"
races = 2500
path = "https://data.typeracer.com/games";
parameters = list(
playerId = str_c("tr:", id),
n = races
)
request = GET(url = path, query = parameters)
View(request)
request$status_code
fromJSON(content(request, 'text'))
as_tibble(fromJSON(content(request, 'text')))
Sys.Date()
races = as_tibble(fromJSON(content(request, 'text'))) %>%
rename(
accuracy = ac,
time = t,
game_num = gn,
racers_num = np,
points = pts
) %>%
select(everything(), ~r, ~sl, ~tid) %>%
mutate(time = as_datetime(time))
races = as_tibble(fromJSON(content(request, 'text'))) %>%
rename(
accuracy = ac,
time = t,
game_num = gn,
racers_num = np,
points = pts
)
View(races)
races = as_tibble(fromJSON(content(request, 'text'))) %>%
rename(
accuracy = ac,
time = t,
game_num = gn,
racers_num = np,
points = pts
) %>%
select(everything(), -r, -sl, -tid) %>%
mutate(time = as_datetime(time))
library(lubridate)
races = as_tibble(fromJSON(content(request, 'text'))) %>%
rename(
accuracy = ac,
time = t,
game_num = gn,
racers_num = np,
points = pts
) %>%
select(everything(), -r, -sl, -tid) %>%
mutate(time = as_datetime(time))
View(races)
write_csv(races, str_c("race_log/", id, ".csv"))
store_races = function(id, races)
{
# Endpoint will feed in id and total races
path = "https://data.typeracer.com/games";
parameters = list(
playerId = str_c("tr:", id),
n = races
)
request = GET(url = path, query = parameters)
# Convert request json to tibble, some renaming for clarity
# General data cleanup, removing uneeded columns and converting datetimes
races = as_tibble(fromJSON(content(request, 'text'))) %>%
rename(
accuracy = ac,
time = t,
game_num = gn,
racers_num = np,
points = pts
) %>%
select(everything(), -r, -sl, -tid) %>%
mutate(time = as_datetime(time))
# Write to csv in race_log folder with id
write_csv(races, str_c("race_log/", id, ".csv"))
return ( NULL )
}
store_races("cheffray", 2500)
names = read_csv("names.csv")
View(names)
names = read_csv("names.csv")
store_races(names$racer, names$races)
x + x
test = function(x) {
x + x
}
test(1:5)
test(names$races)
str_c(x, y)
test = function(x, y) {
str_c(x, y)
}
test(names$racer, names$races)
for (i in seq_along(names)) {
test(names$racer[i], names$races[i])
}
for (i in seq_along(names)) {
names$racer[i]
# test(names$racer[i], names$races[i])
}
seq_along(names)
for (i in nrow(names)) {
names$racer[i]
# test(names$racer[i], names$races[i])
}
nrow(names)
names
for (i in names) {
names
# test(names$racer[i], names$races[i])
}
print(names)
for (i in names) {
print(names)
# test(names$racer[i], names$races[i])
}
for (i in names) {
print(names)
# test(names$racer[i], names$races[i])
}
# names
# for (i in names) {
#   print(names)
#   # test(names$racer[i], names$races[i])
# }
test(names$racer[1:nrow(names)], names$races[1:nrow(names)])
# names
# for (i in names) {
#   print(names)
#   # test(names$racer[i], names$races[i])
# }
store_races(names$racer[1:nrow(names)], names$races[1:nrow(names)])
sql_len(nrow(names))
seq_len(nrow(names))
test = function(x) {
row
}
# names
# for (i in names) {
#   print(names)
#   # test(names$racer[i], names$races[i])
# }
# store_races(names$racer[1:nrow(names)], names$races[1:nrow(names)])
by(names, seq_len(nrow(names)), test(row))
test = function(x) {
row$racer
}
# names
# for (i in names) {
#   print(names)
#   # test(names$racer[i], names$races[i])
# }
# store_races(names$racer[1:nrow(names)], names$races[1:nrow(names)])
by(names, seq_len(nrow(names)), test(row))
?by
?tapply
# names
for (i in nrow(names)) {
print(names)
# test(names$racer[i], names$races[i])
}
print(i)
# names
for (i in nrow(names)) {
print(i)
# test(names$racer[i], names$races[i])
}
# names
for (i in sql_len(nrow(names))) {
print(i)
# test(names$racer[i], names$races[i])
}
# names
for (i in seq_len(nrow(names))) {
print(i)
# test(names$racer[i], names$races[i])
}
# names
for (i in seq_len(nrow(names))) {
test(names$racer[i], names$races[i])
}
test = function(x, y) {
str_c(x, y)
}
# names
for (i in seq_len(nrow(names))) {
test(names$racer[i], names$races[i])
}
t = str_c(x, y)
test = function(x, y) {
t = str_c(x, y)
print(t)
}
# names
for (i in seq_len(nrow(names))) {
test(names$racer[i], names$races[i])
}
names = read_csv("names.csv")
for (i in seq_len(nrow(names))) {
store_races(names$racer[i], names$races[i])
}
setwd("~/Projects/STAT_240/project/draft")
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE,
cache=TRUE, autodep=TRUE, cache.comments=FALSE)
library(tidyverse)
library(httr)
library(rvest)
library(jsonlite)
library(lubridate)
store_races = function(id, races)
{
# Endpoint will feed in id and total races
path = "https://data.typeracer.com/games";
parameters = list(
playerId = str_c("tr:", id),
n = races
)
request = GET(url = path, query = parameters)
# Convert request json to tibble, some renaming for clarity
# General data cleanup, removing uneeded columns and converting datetimes
races = as_tibble(fromJSON(content(request, 'text'))) %>%
rename(
accuracy = ac,
time = t,
game_num = gn,
racers_num = np,
points = pts
) %>%
select(everything(), -r, -sl, -tid) %>%
mutate(time = as_datetime(time))
# Write to csv in race_log folder with id
write_csv(races, str_c("race_log/", id, ".csv"))
# Prevent too many queries at once
Sys.sleep(5)
return ( NULL )
}
test = function(x, y) {
t = str_c(x, y)
print(t)
}
names = read_csv("names.csv")
for (i in seq_len(nrow(names))) {
store_races(names$racer[i], names$races[i])
}
names = read_csv("names.csv")
for (i in seq_len(nrow(names))) {
store_races(names$racer[i], names$races[i])
print(i)
}
